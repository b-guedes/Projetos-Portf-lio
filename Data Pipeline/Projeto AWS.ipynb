{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-guedes/Projetos-Portfolio/blob/main/C%C3%B3pia_de_Project_%7C_Data_Pipeline_with_Telegram_AWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://community.nasscom.in/sites/default/files/styles/960_x_600/public/media/images/Nasscom%20poster%20%284%29.png?itok=b81vGQap)"
      ],
      "metadata": {
        "id": "nzBplcNVR4_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"tabela-conteudo\"></a>\n",
        "# ÍNDICE\n",
        "\n",
        "- [INTRODUÇÃO](#intro)\n",
        "    - [1. Um projeto de Pipeline de dados](#headings)\n",
        "- [CONTEXTO](#contexto)\n",
        "    - [2. Pipeline de dados](#pipeline-de-dados)\n",
        "    - [2.1. Extração dos dados](#extracao-dos-dados)\n",
        "    - [2.2. Dados transacionais e analíticos](#dados-trans-e-ana)\n",
        "- [ARQUITETURA](#arquitetura)\n",
        "    - [3. Ingestão de dados](#ingestao-de-dados)\n",
        "        - [3.1. Amazon Web Services](#amazon-web-services)\n",
        "            - [3.1.1. AWS S3](#aws-s3)\n",
        "            - [3.1.2. AWS Lambda](#aws-lambda)\n",
        "            - [3.1.3. AWS API Gateway](#aws-api-gateway)\n",
        "        - [3.2. Telegram](#telegram)\n",
        "        - [3.3 ETL](#etl)\n",
        "            - [3.3.1. AWS S3](#aws-s3-2)\n",
        "            - [3.3.2. AWS Lambda](#aws-lambda-2)\n",
        "            - [3.3.3. AWS Event Bridge](#aws-event-bridge)\n",
        "- [4. Apresentação dos dados](#apresentacao-dos-dados)\n",
        "    - [4.1. AWS Athena](#aws-athena)\n",
        "    - [4.2. Análise dos dados](#analise-dos-dados)\n",
        "- [CONSIDERAÇÕES FINAIS](#consideracoes-finais)\n"
      ],
      "metadata": {
        "id": "kutgtjdBR4_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"intro\"></a>\n",
        "# **INTRODUÇÃO**"
      ],
      "metadata": {
        "id": "jSug0vxFR4_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"headings\"></a>\n",
        "## 1. **Um projeto de Pipeline de dados**\n",
        "\n",
        "Este trabalho se propõe a executar um pipeline de dados do início ao final deste processo, ou seja, desde montar o meio de coleta dos dados até a sua transformação e análise.\n",
        "\n",
        "Esse projeto é parte da formação de Analista de Dados da EBAC, e tem por objetivo agregar todos os conhecimentos do curso em um único projeto robusto. Ele irá abranger tanto as habilidades de um analista dos dados, como a modelagem de dados ou a organização de insights, quanto a construção de um ambiente em nuvem (através da AWS) capaz de coletar, armazenar e transformar dados, um papel comumente atribuído ao engenheiro de dados.\n",
        "\n",
        "Por tanto, a proposta deste projeto não é propriamente esgotar este conhecimento, mas demonstrar capacidade em dialogar com diferentes níveis de uma organização que se proponha a trabalhar com dados."
      ],
      "metadata": {
        "id": "dP3_tfDpR4_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"contexto\"></a>\n",
        "# **CONTEXTO**"
      ],
      "metadata": {
        "id": "DjWYCHv0R4_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.collidu.com/media/catalog/product/img/5/b/5bb50fefef2484e06f095938298d0ebbd4714c9527ee027038704300392e1029/data-flow-pipeline-slide1.png)"
      ],
      "metadata": {
        "id": "o_8LBA8zR4_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"pipeline-de-dados\"></a>\n",
        "## 2. Pipeline de dados"
      ],
      "metadata": {
        "id": "FE-Jm0ETR4_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de detalhar o projeto, vamos retomar alguns conceitos para que os conceitos fiquem explicitados a todos os públicos. Começando com o que é pipeline de dados e por que ele é importante para o cenário do analista de dados e as organizações atualmente. Em uma definição simples sobre o pipeline, temos que nele\n",
        "\n",
        "> (...) ocorre a ingestão de dados brutos de diversas origens de dados e, em seguida, o transporte deles para o armazenamento, que pode ser um data lake ou um data warehouse, com a finalidade de realizar análises. Antes de serem enviados a um repositório, os dados geralmente passam por algum tipo de processamento. Isso inclui transformações de dados, como filtragem, mascaramento e agregações, que garantem a integração e a padronização de dados apropriadas. Isso é particularmente importante quando o destino do conjunto de dados é um banco de dados relacional. Esse tipo de repositório de dados tem um esquema definido que requer alinhamento, ou seja, colunas e tipos de dados correspondentes, para atualizar os dados existentes com novos dados.\n",
        "[IBM, 2023](https://www.ibm.com/br-pt/topics/data-pipeline)\n",
        "\n",
        "De tal forma, pode ser entendido como os pipeline de dados vêm sendo uma solução cada vez mais utilizada para automatizar, assegurar escala repetitiva de dados, ingestão, transformação e atividades de integração organizacionais. Uma arquitetura bem projetada e adequada de um pipeline de dados pode qualificar e acelerar significativamente a disponibilidade de dados para o consumo das equipes especializadas em uma organização.\n",
        "\n",
        "Assim sendo, vamos aproximar o foco para o nosso projeto e detalhar alguns pontos que dizem respeito ao pipeline que iremos construir aqui. Como vimos acima, os dados podem ser coletados de diversas formas, como, por exemplo, a raspagem de dados em sites, análise de documentos, extração de texto, extração baseada em API, etc."
      ],
      "metadata": {
        "id": "W-3oKz_HR4_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"extracao-dos-dados\"></a>\n",
        "### 2.1. Extração dos dados"
      ],
      "metadata": {
        "id": "HNvhN9ZwR4_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste projeto será utilizada a técnica de extração baseada em uma API. Esta será toda construída no ambiente da ***Amazon Web Services*** **(AWS)** e integrada a um chatbot do aplicativo de mensagens ***Telegram***.\n",
        "\n",
        "O uso do chatbot se justifica por ser uma ferramenta bastante utilizada atualmente no âmbito empresarial, principalmente para a comunicação com os consumidores. Essa interface permite agilizar processos com protocolos bem estabelecidos em que um aplicativo simula e processa uma conversação humana para diversos fins.\n",
        "\n",
        "> Os chatbots podem ser tão simples quanto programas rudimentares que respondem a uma consulta simples com uma resposta de linha única ou tão sofisticados quanto assistentes digitais que aprendem e evoluem para fornecer níveis crescentes de personalização à medida que coletam e processam informações.[Oracle, 2023](https://www.oracle.com/br/chatbots/what-is-a-chatbot/)\n",
        "\n",
        "A utilização de chatbots traz diversos benefícios e usabilidades bastante abrangentes e importantes para os dias atuais. Eles podem aumentar a eficiência operacional e a economia de custos, ao mesmo tempo que simplificam algumas interações mais básicas que clientes possam ter. Além disso, a possibilidade de escalar, personalizar e resolver problemas é importante para o tempo atual.\n",
        "\n",
        "Neste sentido, o projeto usa desta ferramenta para coletar mensagens de um grupo do Telegram para simular uma interação e a capacidade de gravar e transmitir as informações para as próximas etapas."
      ],
      "metadata": {
        "id": "kTyLXfP5R4_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"dados-trans-e-ana\"></a>\n",
        "## 2.2. Dados transacionais e analíticos"
      ],
      "metadata": {
        "id": "aRUp_AXQR4_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez que foi determinado como o projeto irá coletar os dados, é, então, importante distinguir duas etapas que decorrem após este processo, que são os *dados transacionais* e os *dados analíticos*.\n",
        "\n",
        "O primeiro tem como definição acompanhar as interações relacionadas às atividades de uma organização que podem ser\n",
        "> (...) transações comerciais, como pagamentos recebidos de clientes, pagamentos feitos a fornecedores e movimentação de produtos pelo inventário, pedidos feitos ou serviços entregues. Eventos transacionais, que representam as transações em si, normalmente contêm uma dimensão temporal, alguns valores numéricos e referências a outros dados. [Microsoft, 2023](https://learn.microsoft.com/pt-br/azure/architecture/data-guide/relational-data/online-transaction-processing)\n",
        "\n",
        "Assim, esses dados geralmente apresentam transições diárias e em um alto volume, por estas características os dados transacionais podem ser vistos como os dados brutos de uma operação, ou seja, são eles que contêm toda a gama de informações presente em uma organização - dos produtos e inventário à venda e recebimento.\n",
        "\n",
        "Por sua vez, os dados analíticos podem ser entendidos como derivados dos transacionais, isto porque esses dados são um conjunto de informações coletadas e organizadas de forma estruturada para análises que permitam serem criados insights valiosos para as organizações.\n",
        "\n",
        "Este trabalho irá se ater a duas frentes, primeiro consumindo os dados transacionais do chatbot e, posteriormente, transformando e analisando os dados na plataforma da AWS para, assim, apresentar alguns possíveis insights extraídos desses dados."
      ],
      "metadata": {
        "id": "LWPrGM5aR4_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"arquitetura\"></a>\n",
        "# ARQUITETURA"
      ],
      "metadata": {
        "id": "-nZJbIcmR4_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/andre-marcos-perez/data-pipeline-demo/blob/main/docs/image/architecture.png?raw=true)"
      ],
      "metadata": {
        "id": "LdXtt3ecR4_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A forma mais simples e comum de explicar a arquitetura de um pipeline de dados é justamente com a comparação ao sistema de tubulação de água (pipeline em inglês!). A ideia por trás do sistema montado é que, assim como uma tubulação leva a água de um local (um reservatório) para o outro (torneira de uma casa) por meio de um caminho determinado, fazer com que os dados sigam esse mesmo padrão, ou seja, os dados são levados de um reservatório (local onde os dados brutos são coletados), sofrem alterações e são salvos em um destino escolhido. A seguir serão explicitados os principais componentes da arquitetura aqui planejada."
      ],
      "metadata": {
        "id": "dPYtvXXoR4_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"ingestao-de-dados\"></a>\n",
        "## 3. Ingestão de dados"
      ],
      "metadata": {
        "id": "Tu-J2IPFR4_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A etapa de ingestão dos dados acontece quando os dados transacionais passam por ambientes analíticos. De maneira geral, o dado ingerido é transportado no formato mais próximo do original, ou seja, nenhuma transformação é realizada em seu conteúdo ou estrutura (schema).\n",
        "Neste projeto, as mensagens capturadas pelo bot serão ingeridas através da API web de bots do Telegram, as quais são fornecidas no formato JSON. Como o Telegram retem mensagens por apenas 24h em seus servidores, a ingestão via [streaming](https://aws.amazon.com/pt/what-is/streaming-data/) é a mais indicada.\n",
        "Para que esse tipo de ingestão seja possível, vamos utilizar um webhook (gancho web), ou seja, vamos redirecionar as mensagens automaticamente para outra API web - o [AWS API Gateway](https://docs.aws.amazon.com/pt_br/apigateway/latest/developerguide/welcome.html)."
      ],
      "metadata": {
        "id": "MZCX15QMR4_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"amazon-web-services\"></a>\n",
        "### 3.1. Amazon Web Services"
      ],
      "metadata": {
        "id": "y1F7CURFR4_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todos os dados devem ser armazenados em algum lugar e a escolha de como isso deve ser feita também passa pelo projeto, existem hoje diversos serviços capazes de oferecer armazenamento de dados como, por exemplo, [Oracle Cloud](https://www.oracle.com/br/cloud/storage/), [Google Cloud](https://cloud.google.com/products/storage?hl=pt-br), [IBM Cloud Storage](https://www.ibm.com/br-pt/storage?lnk=flathl), [Microsoft Azure](https://azure.microsoft.com/pt-br/products/azure-sql/database/), [AWS Storage](https://aws.amazon.com/pt/products/storage/?nc2=h_ql_prod_st_s), entre outros. Todas essas plataformas oferecem serviços robustos e escaláveis para implementar um projeto de pipeline neles, por isso a escolha de cada um depende da organização e seus objetivos. Neste projeto, utilizaremos a plataforma da Amazon, a **AWS**.\n",
        "\n",
        "Tendo definido onde os dados serão armazenados e ingeridos, devemos configurar o serviço para recebê-los."
      ],
      "metadata": {
        "id": "EYFEWRLXR4_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-s3\"></a>\n",
        "#### 3.1.1. AWS S3"
      ],
      "metadata": {
        "id": "1cHdqFI-R4_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo definido onde os dados serão armazenados e ingeridos, devemos configurar o serviço para recebê-los.\n",
        "Na plataforma da AWS os locais de armazenamento de dados são conhecidos como buckets e se encontram no serviço denominado AWS S3, neles podemos particionar os dados coletados conforme seja do interesse da organização como, por exemplo, pela fonte dos dados ou os dias em que foram coletados.\n",
        "Durante a etapa de ingestão, o AWS S3 tem a função de armazenar passivamente as mensagens captadas pelo bot do Telegram no seu formato original: JSON, ou seja, elas não passam por transformações para serem armazenadas (são os dados transacionais do projeto). Dessa forma, a organização pode atualizar quaisquer dados históricos caso precise fazer ajustes nas tarefas de processamento de dados no futuro."
      ],
      "metadata": {
        "id": "OgtnyiPCR4_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-lambda\"></a>\n",
        "#### 3.1.2 AWS Lambda"
      ],
      "metadata": {
        "id": "srlk663NR4_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo um local para armazenar os dados, na AWS é necessário criar uma função na ferramenta *AWS Lambda* com um código capaz de persistir os dados do bot do Telegram para os buckets. Para tanto vamos criar uma função que opera da seguinte forma:\n",
        "\n",
        " - Recebe a mensagem no parâmetro `event`;\n",
        " - Verifica se a mensagem tem origem no grupo do **Telegram** correto;\n",
        " - Persiste a mensagem no formato JSON no *bucket* do `AWS S3`;\n",
        " - Retorna uma mensagem de sucesso (código de retorno HTTP igual a 200) a API de *bots* do **Telegram**.\n",
        "\n",
        "\n",
        "Esses dados serão recebidos e armazenados no formato JSON dentro dos *buckets* de destino. Além disso, são criadas variáveis de ambiente (\"AWS_S3_BUCKET\" e \"TELEGRAM_CHAT_ID\") e adicionadas permissões (no *AWS IAM*) de escrita para a interação com *AWS S3* e *AWS Lambda*.\n",
        "O código para escrever a função no ***AWS Lambda*** é o que segue:\n"
      ],
      "metadata": {
        "id": "DlQAIFHhR4_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "import boto3\n",
        "\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> dict:\n",
        "\n",
        "  '''\n",
        "  Recebe uma mensagens do Telegram via AWS API Gateway, verifica no\n",
        "  seu conteúdo se foi produzida em um determinado grupo e a escreve,\n",
        "  em seu formato original JSON, em um bucket do AWS S3.\n",
        "  '''\n",
        "\n",
        "  # vars de ambiente\n",
        "\n",
        "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
        "\n",
        "  # vars lógicas\n",
        "\n",
        "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "  filename = f'{timestamp}.json'\n",
        "\n",
        "  # código principal\n",
        "\n",
        "  client = boto3.client('s3')\n",
        "\n",
        "  try:\n",
        "\n",
        "    message = json.loads(event[\"body\"])\n",
        "    chat_id = message[\"message\"][\"chat\"][\"id\"]\n",
        "\n",
        "    if chat_id == TELEGRAM_CHAT_ID:\n",
        "\n",
        "      with open(f\"/tmp/{filename}\", mode='w', encoding='utf8') as fp:\n",
        "        json.dump(message, fp)\n",
        "\n",
        "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
        "\n",
        "  except Exception as exc:\n",
        "      logging.error(msg=exc)\n",
        "      return dict(statusCode=\"500\")\n",
        "\n",
        "  else:\n",
        "      return dict(statusCode=\"200\")"
      ],
      "metadata": {
        "id": "serjiUPlR4_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-api-gateway\"></a>\n",
        "#### 3.1.3. AWS API Gateway"
      ],
      "metadata": {
        "id": "DT8gEY1FR4_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a função Lambda capaz de ingerir os dados, a próxima etapa é criar uma API no *AWS API Gateway* que a conecte à função do AWS Lambda.\n",
        "Na etapa de ingestão, o *AWS API Gateway* tem a função de receber as mensagens captadas pelo bot do Telegram, enviadas via webhook, e iniciar uma função do *AWS Lambda*, passando o conteúdo da mensagem no seu parâmetro event. Para tanto, vamos criar uma API e configurá-la como gatilho da função do *AWS Lambda*:\n",
        "\n",
        " - Acesse o serviço e selecione: *Create API* -> *REST API*;\n",
        " - Insira um nome, como padrão, um que termine com o sufixo `-api`;\n",
        " - Selecione: *Actions* -> *Create Method* -> *POST*;\n",
        " - Na tela de *setup*:\n",
        "  - Selecione *Integration type* igual a *Lambda Function*;\n",
        "  - Habilite o *Use Lambda Proxy integration*;\n",
        "  - Busque pelo nome a função do `AWS Lambda`.\n",
        "Isso gerara uma URL que usaremos na variável \"aws_api_gateway_url\" no código utilizado na etapa de ingestão."
      ],
      "metadata": {
        "id": "fxYfgh5tR4_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"telegram\"></a>\n",
        "### 3.2. Telegram"
      ],
      "metadata": {
        "id": "It8OnFLyR4_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As fontes de dados para um projeto de pipeline podem ser encontradas em muitos lugares, e os próprios dados podem ser de muitas formas. Neste projeto, como foi supracitado, utilizaremos as mensagens trocadas entre usuários e um chatbot do aplicativo de mensagens [Telegram](https://telegram.org/) - a escolha por esse aplicativo se deve pela facilidade com que se pode criar e gerenciar um bot de forma segura e interativa nele.\n",
        "\n",
        "Para criar o bot no aplicativo, o projeto se baseou na documentação de API do Telegram que pode ser acessada [neste](https://core.telegram.org/bots/api) link. A seguir serão detalhados alguns pontos dessa configuração:\n",
        "\n",
        "![The Botfather](https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf)\n",
        "* O bot do Telegram é chamado de *The BotFather*;\n",
        "* Através dele é possível criar um bot com poucos e simples comandos, como mostrado a seguir e utilizado neste projeto:\n",
        "  * 1) No chat com o BotFather digitar: /start\n",
        "  * 2) Depois digitar: /newbot\n",
        "  * 3) Então digitar o nome do bot: < nome_do_bot >\n",
        "  * 4) Após, digitar o nome de usuário do bot: < nickname_para_o_bot >\n",
        "  * 5) E por último ativamos o bot abrindo o chat com ele e digitando: /start\n",
        "* Uma vez criado, o bot gera uma chave única de acesso a ele, a qual será utilizada para a etapa de ingestão de dados e acesso ao conteúdo em que o bot estará responsável por registrar;\n",
        "* Ainda dentro do programa Telegram, foram feitas configurações para garantir as boas práticas no uso de bots. Primeiro deixando exposto na descrição do grupo que todas as mensagens são gravadas, depois tornando o bot um administrador do grupo do qual ele seja designado e, por fim, configuramos o bot para que ele não possa ser adicionado a outros grupos.\n",
        "\n",
        "Tendo sido explicitada a configuração utilizada do bot é possível acessar o conteúdo que ele capta via API, para isso o projeto monta uma URL base e usa alguns métodos URL para interagir com o bot - configurar um webhook através do método *setWebhook* da API do bot do **Telegram** para interagir com os serviços **AWS** que serão detalhados posteriormente. A seguir são apresentados os códigos em Python para isto:"
      ],
      "metadata": {
        "id": "E_Ix3rT5R4_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# informando a chave única gerada pelo bot no Telegram\n",
        "token = getpass()"
      ],
      "metadata": {
        "id": "U2dtQP_jR4_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criando uma URL para o bot - comum a todos os métodos do API\n",
        "base_url = f'https://api.telegram.org/bot{token}'"
      ],
      "metadata": {
        "id": "HHs5pyXjR4_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criando a URL criada na AWS\n",
        "aws_api_gateway_url = getpass()"
      ],
      "metadata": {
        "id": "Z5IYnd6BR4_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#algumas informações sobre a configuração do bot\n",
        "response = requests.get(url=f'{base_url}/setWebhook?url={aws_api_gateway_url}')\n",
        "\n",
        "print(json.dumps(json.loads(response.text), indent=2))"
      ],
      "metadata": {
        "id": "dMDxCF0IR4_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mostrar as informações das mensagens gravadas pelo bot, em formato JSON\n",
        "response = requests.get(url=f'{base_url}/getWebhookInfo')\n",
        "\n",
        "print(json.dumps(json.loads(response.text), indent=2))"
      ],
      "metadata": {
        "id": "E6xy9K2-R4_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após essas etapas, temos os arquivos brutos coletados do Telegram armazenados e particionados no serviço de nuvem da AWS e podem, então, seguir para a próxima etapa do projeto: extração, transformação e carregamento."
      ],
      "metadata": {
        "id": "ejdD2OjdR4_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"table-of-content\"></a>\n",
        "### 3.3 ETL"
      ],
      "metadata": {
        "id": "Yi40OQDQR4_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após coletar os dados e tê-los armazenados em um repositório central (também conhecido como data warehouse), é o momento de extrair esses dados brutos, depois limpá-los e organizá-los para serem mais adequados às análises e, então, carregá-los em um banco de dados de destino. A sigla **[ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load)** representa estas etapas e significa *Extract, Transform and Load* (em português, Extração, Transformação e Carregamento). Abaixo temos graficamente como esse processo ocorre:\n",
        "\n",
        "![Como funciona o processo de ETL?](https://d1.awsstatic.com/whatisimg/Fig1-etljob.feff8a73afe5fbbdb8ebb2f8255c1147deda6106.png)\n",
        "\n",
        "\n",
        "Neste projeto, as mensagens captadas pelo bot em um único dia, serão persistidas na camada crua (como explicitado anteriormente), após isto, elas serão compactadas em um único arquivo. Este será orientado à coluna e comprimido, por fim, ele será persistido em uma camada incremental."
      ],
      "metadata": {
        "id": "Umdx0Ih4R4_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-s3-2\"></a>\n",
        "### 3.3.1. AWS S3"
      ],
      "metadata": {
        "id": "O1ZOGDA7R4_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devemos novamente criar um bucket na AWS S3, mas desta vez ele irá receber os arquivos que forem transformados na etapa de ETL. Como boas práticas, esses locais são chamados de *raw* e *enriched* ou *bronze, silver e gold*, aqui utilizaremos raw e enriched."
      ],
      "metadata": {
        "id": "bTBs2rAyR4_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-lambda-2\"></a>\n",
        "#### 3.3.2. AWS Lambda"
      ],
      "metadata": {
        "id": "wlOD8LAbR4_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este Lambda irá ativamente processar as mensagens coletadas pelo bot e irá persistir para os buckets enriched e fazer a etapa de data wrangling e manipulação dos dados. Para que os processos tenham uma [melhor performasse computacional](https://www.databricks.com/br/glossary/what-is-parquet), os dados serão transformados e armazenados em uma tabela no PyArrow no formato Parquet - do projeto [Apache Hadoop](https://en.wikipedia.org/wiki/Apache_Parquet). OS códigos para isso são os que seguem:"
      ],
      "metadata": {
        "id": "6pL3QdVdR4_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando a função Lambda da segunda camada dos dados (enriched) que irá ingerir os dados na AWS."
      ],
      "metadata": {
        "id": "Yx4z7c-dR4_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import boto3\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> bool:\n",
        "\n",
        "  '''\n",
        "  Diariamente é executado para compactar as diversas mensagensm, no formato\n",
        "  JSON, do dia anterior, armazenadas no bucket de dados cru, em um único\n",
        "  arquivo no formato PARQUET, armazenando-o no bucket de dados enriquecidos\n",
        "  '''\n",
        "\n",
        "  # vars de ambiente\n",
        "\n",
        "  RAW_BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "  ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
        "\n",
        "  # vars lógicas\n",
        "\n",
        "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "  date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "  # código principal\n",
        "\n",
        "  table = None\n",
        "  client = boto3.client('s3')\n",
        "\n",
        "  try:\n",
        "\n",
        "      response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n",
        "\n",
        "      for content in response['Contents']:\n",
        "\n",
        "        key = content['Key']\n",
        "        client.download_file(RAW_BUCKET, key, f\"/tmp/{key.split('/')[-1]}\")\n",
        "\n",
        "        with open(f\"/tmp/{key.split('/')[-1]}\", mode='r', encoding='utf8') as fp:\n",
        "\n",
        "          data = json.load(fp)\n",
        "          data = data[\"message\"]\n",
        "\n",
        "        parsed_data = parse_data(data=data)\n",
        "        iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
        "\n",
        "        if table:\n",
        "\n",
        "          table = pa.concat_tables([table, iter_table])\n",
        "\n",
        "        else:\n",
        "\n",
        "          table = iter_table\n",
        "          iter_table = None\n",
        "\n",
        "      pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n",
        "      client.upload_file(f\"/tmp/{timestamp}.parquet\", ENRICHED_BUCKET, f\"telegram/context_date={date}/{timestamp}.parquet\")\n",
        "\n",
        "      return True\n",
        "\n",
        "  except Exception as exc:\n",
        "      logging.error(msg=exc)\n",
        "      return False"
      ],
      "metadata": {
        "id": "VvD38mizR4_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando a função do data wrangling usada na função anterior."
      ],
      "metadata": {
        "id": "170lRL8cR4_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(data: dict) -> dict:\n",
        "\n",
        "  date = datetime.now().strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "  parsed_data = dict()\n",
        "\n",
        "  for key, value in data.items():\n",
        "\n",
        "      if key == 'from':\n",
        "          for k, v in data[key].items():\n",
        "              if k in ['id', 'is_bot', 'first_name']:\n",
        "                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n",
        "\n",
        "      elif key == 'chat':\n",
        "          for k, v in data[key].items():\n",
        "              if k in ['id', 'type']:\n",
        "                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n",
        "\n",
        "      elif key in ['message_id', 'date', 'text']:\n",
        "          parsed_data[key] = [value]\n",
        "\n",
        "  if not 'text' in parsed_data.keys():\n",
        "    parsed_data['text'] = [None]\n",
        "\n",
        "  return parsed_data"
      ],
      "metadata": {
        "id": "ChicqInvR4_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-event-bridge\"></a>\n",
        "#### 3.3.3. AWS Event Bridge"
      ],
      "metadata": {
        "id": "UzKv3jWDR4_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, configuramos o AWS Event Bridge para que ele carregue todos os dias em determinado horário e o código do Lambda seja executado, funcionando assim como um [scheduler](https://aws.amazon.com/pt/eventbridge/scheduler/). O funcionamento do AWS Event Bridge é representado conforme a seguinte imagem:\n",
        "![Event Bridge](https://d1.awsstatic.com/product-marketing/EventBridge/Product-Page-Diagram_Amazon-EventBridge-Scheduler.ab2cc1a1c0f233a4e1e4c5829a0d6c5fc23d9586.png)"
      ],
      "metadata": {
        "id": "pFLt_pcwR4_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"apresentacao-dos-dados\"></a>\n",
        "## 4. Apresentação dos dados"
      ],
      "metadata": {
        "id": "b6oqN0GIR4_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A última etapa do projeto é onde os dados coletados são entregues para os usuários (analistas, cientistas, etc) e sistemas (dashboards, motores de consulta, etc) para que eles sejam utilizados. Este projeto opta por utilizar a linguagem em [SQL](https://pt.wikipedia.org/wiki/SQL) pelo seu fácil e abrangente uso dentro do mercado de dados. Além disso, nesta etapa, é importante entregar os dados de forma refinada, para que as consultas não precisem ser muito custosas, assim os dados são, também, mais consistentes - por terem de lidar com menos dados desnecessários.\n",
        "Nesta etapa, iremos construir uma tabela com os dados de maior interesse na ferramenta AWS Athena e, após esta etapa, poderão ser realizadas as consultas analíticas desejadas. A seguir será demonstrado como isto foi feito."
      ],
      "metadata": {
        "id": "DS3FmkzRR4_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"aws-athena\"></a>\n",
        "### 4.1. AWS Athena"
      ],
      "metadata": {
        "id": "-Wnfgt_OR4_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na etapa de apresentação, o AWS Athena tem função de entregar os dados por uma interface SQL para os usuários do sistema analítico. Para criar a interface, basta criar uma tabela externa sobre o dado armazenado na camada mais refinada da arquitetura, a camada enriquecida. A construção da tabela foi feita como segue:"
      ],
      "metadata": {
        "id": "RpHmrBu8R4_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "CREATE EXTERNAL TABLE `telegram`(\n",
        "  `message_id` bigint,\n",
        "  `user_id` bigint,\n",
        "  `user_is_bot` boolean,\n",
        "  `user_first_name` string,\n",
        "  `chat_id` bigint,\n",
        "  `chat_type` string,\n",
        "  `text` string,\n",
        "  `date` bigint)\n",
        "PARTITIONED BY (\n",
        "  `context_date` date)\n",
        "ROW FORMAT SERDE\n",
        "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
        "STORED AS INPUTFORMAT\n",
        "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n",
        "OUTPUTFORMAT\n",
        "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
        "LOCATION\n",
        "  's3://<bucket-enriquecido>/'\n",
        "```"
      ],
      "metadata": {
        "id": "P7qcQ7_LR4_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como o monitoramento foi feito em mais de um dia, devemos adicionar as diferentes partições dos dados a uma única tabela, para isso é preciso usar:"
      ],
      "metadata": {
        "id": "Af59CqCdR4_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para adicionar todas as partições:\n",
        "\n",
        "```sql\n",
        "MSCK REPAIR TABLE `telegram`;\n",
        "```"
      ],
      "metadata": {
        "id": "OdT6XPjjR4_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ou para determinar quais partições serão adicionadas:\n",
        "\n",
        "```sql\n",
        "ALTER TABLE <nome-tabela> ADD PARTITION <coluna-partição> = <valor-partição>\n",
        "```"
      ],
      "metadata": {
        "id": "tx2H8yQlR4_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a tabela construida podemos ver um pouco melhor como são os dados utilizados, eles podem ser descritos desta maneira:\n",
        "\n",
        "| chave | tipo valor | opcional | descrição |\n",
        "| -- | -- | -- | -- |\n",
        "| message_id | int | não | id da mensagem enviada ao grupo |\n",
        "| updated_id | int | não | id da mensagem enviada ao **bot** |\n",
        "| user_id | int | sim | id do usuário que enviou a mensagem |\n",
        "| user_is_bot | bool | sim | se o usuário que enviou a mensagem é um **bot** |\n",
        "| user_first_name | str | sim | primeiro nome do usário que enviou a mensagem |\n",
        "| chat_id | int | não | id do *chat* em que a mensagem foi enviada |\n",
        "| chat_type | str | não | tipo do *chat*: private, group, supergroup ou channel |\n",
        "| text | str | sim | texto da mensagem |\n",
        "| date | int | não | data de envio da mensagem no formato unix |\n"
      ],
      "metadata": {
        "id": "GE4dStE7R4_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"analise-dos-dados\"></a>\n",
        "### 4.2. Análise dos dados"
      ],
      "metadata": {
        "id": "VMIMhglnR4_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste momento temos duas opções para seguir com o projeto, sendo elas também cumulativas, continuamos na linguagem SQL e realizamos consultas através disto, ou criamos um arquivo para analisar em outro local, como um ambiente em Python ou de visualização de dados, como o Power BI. Não tendo aqui o objetivo de esgotar as possibilidades, colocaremos apenas alguns exemplos de como isso pode ser feito."
      ],
      "metadata": {
        "id": "iCLEwFKlR4_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando consultas básicas como:\n",
        "* O total de mensagens por usuário"
      ],
      "metadata": {
        "id": "y1usjuajR4_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "SELECT user_id, COUNT(1) AS amount_msg\n",
        "FROM telegram\n",
        "GROUP BY user_id;\n",
        "```"
      ],
      "metadata": {
        "id": "tRPXJc0dR4_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando função e agregação de datas como:\n",
        "* A quantidade de dias entre a primeira e a última mensagem de cada usuário"
      ],
      "metadata": {
        "id": "FRKlslA8R4_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "SELECT user_id, EXTRACT(DAY FROM (MAX(context_date) - MIN(context_date))) AS \"days_between\"\n",
        "FROM telegram\n",
        "GROUP BY user_id;\n",
        "```"
      ],
      "metadata": {
        "id": "j5S53AueR4_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* O total de mensagens de cada usuário, agrupado por dia"
      ],
      "metadata": {
        "id": "fAGozrIKR4_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "SELECT\n",
        "  user_id,\n",
        "  context_date,\n",
        "  count(1) AS \"message_amount\"\n",
        "FROM \"telegram\"\n",
        "GROUP BY\n",
        "  user_id,\n",
        "  context_date\n",
        "ORDER BY context_date DESC\n",
        "```"
      ],
      "metadata": {
        "id": "snWIK9r1R4_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando agrupamento e agregação como:\n",
        "* O total de caracteres das mensagens por dia, de cada usuário."
      ],
      "metadata": {
        "id": "AwZujsp4R4_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "SELECT\n",
        "  user_id,\n",
        "  context_date,\n",
        "  CAST(AVG(length(text)) AS INT) AS \"average_message_length\"\n",
        "FROM \"telegram\"\n",
        "GROUP BY\n",
        "  user_id,\n",
        "  context_date\n",
        "ORDER BY context_date DESC\n",
        "```"
      ],
      "metadata": {
        "id": "UztQKrsDR4_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando Common Table Expression (CTE) e extração de data/hora:\n",
        "* Mensagens agrupadas por hora e dias da semana"
      ],
      "metadata": {
        "id": "m4ZoPMPNR4_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```sql\n",
        "WITH\n",
        "parsed_date_cte AS (\n",
        "    SELECT\n",
        "        *,\n",
        "        CAST(date_format(from_unixtime(\"date\"),'%Y-%m-%d %H:%i:%s') AS timestamp) AS parsed_date\n",
        "    FROM \"telegram\"\n",
        "),\n",
        "hour_week_cte AS (\n",
        "    SELECT\n",
        "        *,\n",
        "        EXTRACT(hour FROM parsed_date) AS parsed_date_hour,\n",
        "        EXTRACT(dow FROM parsed_date) AS parsed_date_weekday\n",
        "    FROM parsed_date_cte\n",
        ")\n",
        "SELECT\n",
        "    parsed_date_hour,\n",
        "    parsed_date_weekday,\n",
        "    count(1) AS \"message_amount\"\n",
        "FROM hour_week_cte\n",
        "GROUP BY\n",
        "    parsed_date_hour,\n",
        "    parsed_date_weekday\n",
        "ORDER BY\n",
        "    parsed_date_weekday\n",
        "```"
      ],
      "metadata": {
        "id": "agiBcCFlR4_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfim, esses são apenas alguns exemplos de como podem ser extraídas informações valiosas de dados através da consulta com SQL, as possibilidades são infinitas e devem ser adequadas aos objetivos dos usuários e das organizações.\n",
        "\n",
        "É interessante destacar ainda que é possível extrair qualquer resultado para o formato CSV e assim utilizá-lo em outras plataformas também."
      ],
      "metadata": {
        "id": "fHMifGpmR4_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como exemplo, podemos demonstrar que, ao consultarmos a tabela inteira através da seguinte expressão:\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM telegram;\n",
        "```\n",
        "\n",
        "Com a consulta feita, podemos salvar o arquivo no formato CSV e assim importá-lo para outra plataforma, como, por exemplo, o Power BI - como no modelo semântico seguir:"
      ],
      "metadata": {
        "id": "pL5wH8IiR4_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/b-guedes/ebacFIGS/blob/main/consulta8.png?raw=true)"
      ],
      "metadata": {
        "id": "qkqcSKMkR4_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um exemplo de dashboard que poderia ser criado:\n",
        "\n",
        "![image](https://github.com/b-guedes/ebacFIGS/blob/main/powerbi_exemplo.png?raw=true)\n"
      ],
      "metadata": {
        "id": "CTcsSGpYR4_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo deste projeto não é se aprofundar nas possibilidades de extração dos insights ou apresentação dos dados, mas demonstrar como eles poderiam seguir por esse caminho e gerar resultados que interessem os usuários e organizações do pipeline de dados."
      ],
      "metadata": {
        "id": "nbCvyAXrR4_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"consideracoes-finais\"></a>\n",
        "# Considerações finais"
      ],
      "metadata": {
        "id": "nHHdv2YPR4_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao final desse projeto, ficou demonstrado como os dados podem ser usados hoje em dia pelas organizações e como eles estão cada vez mais acessíveis, tanto com a profusão de ferramentas e plataformas existentes como com a redução da complexidade em implementar essas estruturas.\n",
        "\n",
        "Em um contexto de organizações que o volume de dados seja muito grande, é de se pressupor que a complexidade desses projetos cresça e também o envolvimento de mais especialistas em cada área, como os engenheiros de dados, cientistas de dados e os analistas de dados, cada um com uma função específica e necessária em uma grande organização. Ainda assim, é importante que alguém na ponta desse processo tenha em mente como, em linhas gerais, o dado chega para ser analisado e este era o objetivo deste projeto.\n",
        "\n",
        "Por fim, é de se destacar como a documentação para aplicação de cada etapa é muito divulgada e de fácil acesso, disponibilizada pelas próprias plataformas para que os usuários possam implementar as tecnologias da melhor forma possível. A intuitividade na montagem de projetos assim parece ser uma tendência em um mundo de crescente aperfeiçoamento das IAs ou de montagem de sistemas complexos de Machine Learn com técnicas simples, como no projeto [Visual Blocks](https://visualblocks.withgoogle.com/) da Google."
      ],
      "metadata": {
        "id": "rn9bJJsxR4_5"
      }
    }
  ]
}
